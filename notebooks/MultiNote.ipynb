{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/presetgen/multioutputnormed/c0e0c1d85d714853801e918d222af116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Comet and create experiment\n",
    "from comet_ml import Experiment\n",
    "\n",
    "# Comet variables\n",
    "from tensorflow.python.keras import Input, Model\n",
    "from tensorflow.python.keras.layers import concatenate\n",
    "\n",
    "API_KEY = \"gKo0lOrCw6burIxXrjggq8OMY\"\n",
    "PROJECT = \"MultiOutputNormed\"\n",
    "WORKSPACE = \"PresetGen\"\n",
    "VSTPARAMETER = 25\n",
    "\n",
    "experiment = Experiment(api_key=API_KEY, project_name=PROJECT, workspace=WORKSPACE, log_code=True)\n",
    "experiment.set_code()\n",
    "# Dependencies\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "import keras.layers as layers\n",
    "import kapre as kapre\n",
    "import joblib as joblib\n",
    "\n",
    "# matplotlib.use('agg')\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # no graka...\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mfccs5Second_Env10_MultipleBatchMinMax07152020145158\n"
     ]
    }
   ],
   "source": [
    "ACTUALEXNAME = \"mfccs5Second_Env10_MultipleBatchMinMax\" + datetime.now().strftime(\"%m%d%Y%H%M%S\")\n",
    "print(ACTUALEXNAME)\n",
    "# Dependencies\n",
    "SR = 22050\n",
    "\n",
    "\n",
    "def getOneLinerMfcc(mfcc):\n",
    "    new = np.mean(mfcc, axis=0)\n",
    "    return new\n",
    "\n",
    "\n",
    "def extract_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccsscaled = np.mean(mfccs.T, axis=0)\n",
    "        SR = sample_rate\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n",
    "\n",
    "    return mfccsscaled\n",
    "\n",
    "\n",
    "def getAudio(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "        SR = sample_rate\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None\n",
    "\n",
    "    return audio\n",
    "\n",
    "\n",
    "def Conv1D(SR=SR, DT=1.0):\n",
    "    i = layers.Input(shape=(1, int(SR * DT)), name='input')\n",
    "    x = kapre.Melspectrogram(n_dft=512, n_hop=160,\n",
    "                             padding='same', sr=SR, n_mels=128,\n",
    "                             fmin=0.0, fmax=SR / 2, power_melgram=2.0,\n",
    "                             return_decibel_melgram=True, trainable_fb=False,\n",
    "                             trainable_kernel=False,\n",
    "                             name='melbands')(i)\n",
    "    x = kapre.Normalization2D(str_axis='batch', name='batch_norm')(x)\n",
    "    x = layers.Permute((2, 1, 3), name='permute')(x)\n",
    "    x = kapre.TimeDistributed(layers.Conv1D(8, kernel_size=(4), activation='tanh'), name='td_conv_1d_tanh')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2), name='max_pool_2d_1')(x)\n",
    "    x = kapre.TimeDistributed(layers.Conv1D(16, kernel_size=(4), activation='relu'), name='td_conv_1d_relu_1')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2), name='max_pool_2d_2')(x)\n",
    "    x = kapre.TimeDistributed(layers.Conv1D(32, kernel_size=(4), activation='relu'), name='td_conv_1d_relu_2')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2), name='max_pool_2d_3')(x)\n",
    "    x = kapre.TimeDistributed(layers.Conv1D(64, kernel_size=(4), activation='relu'), name='td_conv_1d_relu_3')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2), name='max_pool_2d_4')(x)\n",
    "    x = kapre.TimeDistributed(layers.Conv1D(128, kernel_size=(4), activation='relu'), name='td_conv_1d_relu_4')(x)\n",
    "    x = layers.GlobalMaxPooling2D(name='global_max_pooling_2d')(x)\n",
    "    x = layers.Dropout(rate=0.1, name='dropout')(x)\n",
    "    x = layers.Dense(64, activation='relu', activity_regularizer=keras.regularizers.l2(0.001), name='dense')(x)\n",
    "    o = layers.Dense(1, name='dense_output')(x)\n",
    "\n",
    "    model1 = kapre.Model(inputs=i, outputs=o, name='1d_convolution')\n",
    "\n",
    "    return model1\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Data Appended\n"
     ]
    }
   ],
   "source": [
    "# Set the path to the full UrbanSound dataset\n",
    "fulldatasetpath = \"I://Synth1PresetTestFiles\"\n",
    "\n",
    "features_mfcc = []\n",
    "features_env10 = []\n",
    "\n",
    "testserializer = []\n",
    "\n",
    "dirFiles = os.listdir(\"I://Synth1PresetTestFiles\")\n",
    "dirFiles.sort(key=lambda f: int(re.sub('\\D', '', f)))\n",
    "env10 = joblib.load(\"I://EnvelopeDiv10.dat\")\n",
    "# mfccss = []\n",
    "# mfccss = np.load(\"I://zeroCrossFromPreset_frame64hop16.dat\", allow_pickle=True)\n",
    "mfccss1 = joblib.load(\"I://mfccFromPreset_128fft256hop64_1.dat\")\n",
    "mfccss2 = joblib.load(\"I://mfccFromPreset_128fft256hop64_2.dat\")\n",
    "mfccss = np.append(mfccss1, mfccss2)[1::2]\n",
    "del mfccss1, mfccss2\n",
    "\n",
    "value = 0\n",
    "\n",
    "for file in dirFiles:\n",
    "    if \"xml\" in file:\n",
    "        sampleNumber = int(file.split(\".\")[0][4:])\n",
    "        tempName = file.split(\".\")[0] + \".wav\"\n",
    "        fileName = os.path.join(os.path.abspath(fulldatasetpath), tempName)\n",
    "        tree = ET.parse(os.path.join(os.path.abspath(fulldatasetpath), file))\n",
    "        root = tree.getroot()\n",
    "        classlabels = []\n",
    "        for x in range(99):\n",
    "            classlabels.append(int(root[x + 2].attrib[\"presetValue\"]))\n",
    "        #        classLabel = int(root[value + 2].attrib[\"presetValue\"])\n",
    "        data_mfcc = mfccss[sampleNumber]\n",
    "        data_env10 = env10[sampleNumber, 1]\n",
    "        features_mfcc.append([data_mfcc, classlabels])\n",
    "        features_env10.append([data_env10, classlabels])\n",
    "        value = value + 1\n",
    "        # print(\"Appended \" + fileName + \" Class = \" + str(classlabels) + \" SampleNumber = \" + str(sampleNumber))\n",
    "\n",
    "del dirFiles\n",
    "print(\"All Data Appended\")\n",
    "# np.array(mfccss).dump(\"mfccsFromPreset.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 10.5 GiB for an array with shape (12772, 128, 1723) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-8-759093b9722e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[1;32mdel\u001B[0m \u001B[0mfeatures_mfcc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeatures_env10\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;32mdel\u001B[0m \u001B[0mmfccss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtree\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mroot\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 28\u001B[1;33m \u001B[0mx_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_test\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfeatureArray_mfcc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalueArray\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m42\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     29\u001B[0m \u001B[0mx_train2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx_test2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_test2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfeatureArray_env10\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalueArray\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrandom_state\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m42\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\repos\\tensortest\\virtualenv\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001B[0m in \u001B[0;36mtrain_test_split\u001B[1;34m(*arrays, **options)\u001B[0m\n\u001B[0;32m   2153\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2154\u001B[0m     return list(chain.from_iterable((_safe_indexing(a, train),\n\u001B[1;32m-> 2155\u001B[1;33m                                      _safe_indexing(a, test)) for a in arrays))\n\u001B[0m\u001B[0;32m   2156\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2157\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\repos\\tensortest\\virtualenv\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001B[0m in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   2153\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2154\u001B[0m     return list(chain.from_iterable((_safe_indexing(a, train),\n\u001B[1;32m-> 2155\u001B[1;33m                                      _safe_indexing(a, test)) for a in arrays))\n\u001B[0m\u001B[0;32m   2156\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2157\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\repos\\tensortest\\virtualenv\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001B[0m in \u001B[0;36m_safe_indexing\u001B[1;34m(X, indices, axis)\u001B[0m\n\u001B[0;32m    393\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0m_pandas_indexing\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindices\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindices_dtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    394\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"shape\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 395\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0m_array_indexing\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindices\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindices_dtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    396\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    397\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0m_list_indexing\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindices\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mindices_dtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\repos\\tensortest\\virtualenv\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001B[0m in \u001B[0;36m_array_indexing\u001B[1;34m(array, key, key_dtype, axis)\u001B[0m\n\u001B[0;32m    179\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    180\u001B[0m         \u001B[0mkey\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 181\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0marray\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0maxis\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0marray\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    182\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    183\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 10.5 GiB for an array with shape (12772, 128, 1723) and data type float32"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert into a Panda dataframe\n",
    "features_mffc_new = pd.DataFrame(features_mfcc, columns=['feature', 'class_label'])\n",
    "features_env10_new = pd.DataFrame(features_env10, columns=['feature', 'class_label'])\n",
    "\n",
    "\n",
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "featureArray_mfcc = np.array(features_mffc_new.feature.tolist())\n",
    "featureArray_env10 = np.array(features_env10_new.feature.tolist())\n",
    "valueArray = np.array(features_mffc_new.class_label.tolist())\n",
    "# valueArray = tf.keras.utils.normalize(valueArray, axis=0, order=2)\n",
    "# min_d = np.min(valueArray)\n",
    "# max_d = np.max(valueArray)\n",
    "# valueArray = (valueArray - min_d) / (max_d - min_d)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(valueArray)\n",
    "valueArray = scaler.transform(valueArray)\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "# yy = to_categorical(le.fit_transform(valueArray))\n",
    "\n",
    "# split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "del features_mfcc, features_env10\n",
    "del mfccss, tree, root\n",
    "x_train, x_test, y_train, y_test = train_test_split(featureArray_mfcc, valueArray, test_size=0.2, random_state=42)\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(featureArray_env10, valueArray, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_mfcc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-11-e10ac9587056>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mdel\u001B[0m \u001B[0mfeatures_mfcc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfeatures_env10\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mdel\u001B[0m \u001B[0mmfccss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtree\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mroot\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m#del featureArray_mfcc, featureArray_env10, valueArray\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mfeatures_mffc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'features_mfcc' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "del features_mfcc, features_env10\n",
    "del mfccss, tree, root\n",
    "#del featureArray_mfcc, featureArray_env10, valueArray\n",
    "\n",
    "features_mffc = 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "num_labels = valueArray.shape\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model\n",
    "# model = Sequential([\n",
    "#     Dense(16, activation=tf.nn.leaky_relu, input_shape=(40, 988,)),\n",
    "#     Dense(32, activation=tf.nn.leaky_relu),\n",
    "#     layers.GlobalMaxPool1D(),\n",
    "#     Dense(64, activation=tf.nn.leaky_relu),\n",
    "#     Dense(1)\n",
    "# ])\n",
    "# model = Sequential([\n",
    "#     Dense(16, activation=tf.nn.leaky_relu, input_shape=(128, 1723,)),\n",
    "#     Dense(32, activation=tf.nn.leaky_relu),\n",
    "#     layers.GlobalMaxPool1D(),\n",
    "#     Dense(64, activation=tf.nn.leaky_relu),\n",
    "#     Dense(99)\n",
    "# ])\n",
    "# model = Sequential()\n",
    "# model.add(Dense(16, input_shape=((128, 1723,), (11025,)), activation=\"relu\"))\n",
    "# # model.add(keras.layers.BatchNormalization())\n",
    "# # model.add(layers.Activation(\"relu\"))\n",
    "# model.add(Dense(32, activation=\"relu\"))\n",
    "# model.add(layers.GlobalMaxPool1D())\n",
    "# model.add(Dense(64, activation=tf.nn.leaky_relu))\n",
    "# model.add(Dense(99))\n",
    "\n",
    "# define two sets of inputs\n",
    "inputA = Input(shape=(128, 1723,))\n",
    "inputB = Input(shape=(11025,))\n",
    "# the first branch operates on the first input\n",
    "x = Dense(16, activation=\"relu\")(inputA)\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "x = Dense(64, activation=tf.nn.leaky_relu)(x)\n",
    "#x = Dense(99)(x)\n",
    "x = Model(inputs=inputA, outputs=x)\n",
    "\n",
    "# the second branch opreates on the second input\n",
    "y = Dense(16, activation=\"relu\")(inputB)\n",
    "y = Dense(32, activation=\"relu\")(y)\n",
    "y = Dense(64, activation=tf.nn.leaky_relu)(y)\n",
    "#y = Dense(99)(y)\n",
    "y = Model(inputs=inputB, outputs=y)\n",
    "\n",
    "# combine the output of the two branches\n",
    "combined = concatenate([x.output, y.output])\n",
    "# apply a FC layer and then a regression prediction on the\n",
    "# combined outputs\n",
    "z = Dense(2, activation=\"relu\")(combined)\n",
    "z = Dense(99)(z)\n",
    "# our model will accept the inputs of the two branches and\n",
    "# then output a single value\n",
    "model = Model(inputs=[x.input, y.input], outputs=z)\n",
    "\n",
    "print(model.summary())\n",
    "optimizer = optimizers.RMSprop(0.001)\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', 'mae'])\n",
    "\n",
    "num_epochs = 100\n",
    "num_batch_size = 32\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "# model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), verbose=1)\n",
    "# callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "# reduce_lr_callback = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=10, min_lr=0.0001, verbose=1)\n",
    "\n",
    "# my_callbacks = [\n",
    "#     tf.keras.callbacks.EarlyStopping(patience=2),\n",
    "#     tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "#     tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "# ]\n",
    "print(\"start Training....\")\n",
    "history = model.fit(\n",
    "    [x_train, x_train2], y_train, batch_size=32,\n",
    "    epochs=num_epochs, validation_split=0.2, verbose=0) # validation_data=(x_test, y_test)\n",
    "\n",
    "duration = datetime.now() - start\n",
    "model.save(ACTUALEXNAME)\n",
    "print(model.summary())\n",
    "\n",
    "print(\"Training completed in time: \", duration)\n",
    "\n",
    "print(ACTUALEXNAME)\n",
    "\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print('train accuracy: {}'.format(score))\n",
    "# experiment.log_metric(\"train_acc\", score)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('test accuracy: {}'.format(score))\n",
    "# experiment.log_metric(\"val_acc\", score)\n",
    "\n",
    "# print(\"First 10 Value Predictions\")\n",
    "# exampleMfccs = x_test[:10]\n",
    "#\n",
    "# exampleresult = model.predict(exampleMfccs)\n",
    "#\n",
    "# for val in exampleresult:\n",
    "#     print(val)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}